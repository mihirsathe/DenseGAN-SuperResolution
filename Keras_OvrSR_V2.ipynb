{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mihirsathe/ECE285FA18_BestGroup/blob/master/Keras_OvrSR_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "id": "etiSuCAVNfhL",
    "outputId": "8fb95154-1820-4416-8892-5d1cf8ff4113"
   },
   "outputs": [],
   "source": [
    "#!pip3 install -q git+https://www.github.com/keras-team/keras-contrib.git\n",
    "#!pip3 install -q DataLoader\n",
    "#!pip3 install -q imageio\n",
    "#!pip3 install -q keras\n",
    "\n",
    "#Mount google drive with dataset\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "dir_pfx = './'\n",
    "data_dir = '../data/Vehicules1024/'\n",
    "# keras.backend.set_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xaMsQct9Lxem",
    "outputId": "2857e543-c53c-4e80-858f-73e41622191e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Super-resolution of CelebA using Generative Adversarial Networks.\n",
    "# The dataset can be downloaded from: https://www.dropbox.com/sh/8oqt9vytwxb3s4r/AADIKlz8PR9zr6Y20qbkunrba/Img/img_align_celeba.zip?dl=0\n",
    "# Instrustion on running the script:\n",
    "# 1. Download the dataset from the provided link\n",
    "# 2. Save the folder 'img_align_celeba' to 'datasets/'\n",
    "# 4. Run the sript using command 'python srgan.py'\n",
    "\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import scipy\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Add\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.applications import VGG19\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# from google.col\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import keras.backend as K\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M6s0jXbDLqgV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "from keras.utils import Sequence\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from skimage.transform import downscale_local_mean\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "  return image/255.0    \n",
    "\n",
    "def read_VEDAI(subset, PATH_TO_VEHICLES_FOLDER):\n",
    "    # Takes in the full path to the unzipped \"VEHICULES\" folder\n",
    "    # Returns mapping dict, RGB and Infrared images in \n",
    "    # (images, x, y, channels) format,\n",
    "    # saves a txt file with mapping of rgb/infra idx to filename\n",
    "    # NUM_FILES = 2536\n",
    "    # MAX_INDEX = 1272\n",
    "    X_PIXELS = 1024\n",
    "    Y_PIXELS = 1024\n",
    "    PATH_TO_VEHICLES_FOLDER = PATH_TO_VEHICLES_FOLDER#[0]\n",
    "\n",
    "    onlyfiles = [f for f in listdir(PATH_TO_VEHICLES_FOLDER) if isfile(\n",
    "        join(PATH_TO_VEHICLES_FOLDER, f)) and \"png\" in f]\n",
    "    # assert len(onlyfiles) == NUM_FILES, \"Not the full VEDAI 1024 Dataset\"\n",
    "    rgb = np.zeros((len(subset), X_PIXELS, Y_PIXELS, 3))\n",
    "    infra = np.zeros((len(subset), X_PIXELS, Y_PIXELS, 1))\n",
    "    indices = subset\n",
    "\n",
    "    print(indices)\n",
    "    print(rgb.shape)\n",
    "    index_filename_map = {}\n",
    "    im_cnt = 0\n",
    "\n",
    "    for index in indices:\n",
    "        pair = [file for file in onlyfiles if str(index) in file]\n",
    "        if pair:\n",
    "            for file in pair:\n",
    "                index_filename_map[im_cnt] = file.split('_')[0]\n",
    "                im = imageio.imread(PATH_TO_VEHICLES_FOLDER + '/' + file)\n",
    "                if \"co\" in file:\n",
    "                    # print('Inserting RGB @ '+ str(im_cnt))\n",
    "                    rgb[im_cnt, :, :, :] = np.reshape(\n",
    "                        im, (tuple([1]) + im.shape))\n",
    "                elif \"ir\" in file:\n",
    "                    # print('Inserting Infra @ '+ str(im_cnt))\n",
    "                    infra[im_cnt, :, :, :] = np.reshape(\n",
    "                        im, (tuple([1]) + im.shape + tuple([1])))\n",
    "            im_cnt = im_cnt + 1\n",
    "        else:\n",
    "            print(\"The following image is missing!: \" + index)\n",
    "    # print(index_filename_map)\n",
    "    f = open(PATH_TO_VEHICLES_FOLDER + \"_mapping.txt\", \"w\")\n",
    "    f.write(str(index_filename_map))\n",
    "    f.close()\n",
    "    return rgb, infra\n",
    "\n",
    "\n",
    "def scan_dataset(PATH_TO_VEHICLES_FOLDER, number_of_imgs):\n",
    "    # Takes in the full path to the unzipped \"VEHICULES\" folder\n",
    "    # Returns a list of all the files\n",
    "    # and saves a dataset summary text file with list of all file names\n",
    "    MAX_INDEX = number_of_imgs\n",
    "    PATH_TO_VEHICLES_FOLDER = PATH_TO_VEHICLES_FOLDER#[0]\n",
    "    indices = [format(n, '08') for n in range(MAX_INDEX)]\n",
    "    export_files = []\n",
    "\n",
    "    onlyfiles = [f for f in listdir(PATH_TO_VEHICLES_FOLDER) if isfile(\n",
    "        join(PATH_TO_VEHICLES_FOLDER, f)) and \"png\" in f]\n",
    "\n",
    "    for index in indices:\n",
    "        pair = [file for file in onlyfiles if str(index) in file]\n",
    "        if pair:\n",
    "            for file in pair:\n",
    "                if \"co\" in file:\n",
    "                    export_files.append(file.split('_')[0])\n",
    "        else:\n",
    "            print(\"The following image is missing!: \" + index)\n",
    "    np.savetxt(PATH_TO_VEHICLES_FOLDER + '_summary.txt',\n",
    "               export_files, delimiter=\" \", fmt=\"%s\")\n",
    "    return export_files\n",
    "\n",
    "\n",
    "def create_subsets(imgs, output_path, use_validation=True,\n",
    "                   training_percent=0.7, testing_percent=0.3, SEED=1):\n",
    "    # Takes a list of image file names and shuffles\n",
    "    # them before splitting them into required subsets\n",
    "    # Saves txt files containing the names of the files\n",
    "    # used in each subset, no return value\n",
    "\n",
    "    assert training_percent + \\\n",
    "        testing_percent == 1, \"Training + testing percents must equal 1.\"\n",
    "    random.seed(SEED)\n",
    "    random.shuffle(imgs)\n",
    "    print('Using ' + str(len(imgs)) + ' images.')\n",
    "    print('Saving files to ' + output_path)\n",
    "    if not use_validation:\n",
    "        training_imgs = imgs[:int(len(imgs) * training_percent)]\n",
    "        testing_imgs = imgs[int(len(imgs) * training_percent):]\n",
    "        np.savetxt(output_path + 'training.txt',\n",
    "                   training_imgs, delimiter=\" \", fmt=\"%s\")\n",
    "        np.savetxt(output_path + 'testing.txt',\n",
    "                   testing_imgs, delimiter=\" \", fmt=\"%s\")\n",
    "        return training_imgs, testing_imgs\n",
    "    else:\n",
    "        validation_split = 0.3  # use 30% of training dataset for validation\n",
    "        training_imgs = imgs[int(len(imgs) * validation_split *\n",
    "                                 training_percent):int(len(imgs) *\n",
    "                                                       training_percent)]\n",
    "        validation_imgs = imgs[:int(\n",
    "            len(imgs) * validation_split * training_percent)]\n",
    "        testing_imgs = imgs[int(len(imgs) * training_percent):]\n",
    "        np.savetxt(output_path + 'validation.txt',\n",
    "                   validation_imgs, delimiter=\" \", fmt=\"%s\")\n",
    "        np.savetxt(output_path + 'training.txt',\n",
    "                   testing_imgs, delimiter=\" \", fmt=\"%s\")\n",
    "        np.savetxt(output_path + 'testing.txt',\n",
    "                   testing_imgs, delimiter=\" \", fmt=\"%s\")\n",
    "        return training_imgs, validation_imgs, testing_imgs\n",
    "\n",
    "\n",
    "def save_VEDAI(rgb, infra):\n",
    "    # Takes in arrays of rgb and infrared images\n",
    "    # Saves them to disk, no return value\n",
    "    np.save(\"vedai_rgb_all.npy\", rgb)\n",
    "    np.save(\"vedai_infra_all.npy\", infra)\n",
    "\n",
    "\n",
    "def load_VEDAI():\n",
    "    # No parameters, expected to run in directory with VEDAI.npy files\n",
    "    # Returns two arrays with rgb and infrared images respectively\n",
    "    rgb = np.load(\"vedai_rgb_all.npy\")\n",
    "    infra = np.load(\"vedai_infra_all.npy\")\n",
    "    return rgb, infra\n",
    "\n",
    "\n",
    "def data_explore(data):\n",
    "    print(\"Shape of the data is\" + str(data.shape))\n",
    "    print(\"Dtype of the data is\" + str(data.dtype))\n",
    "\n",
    "\n",
    "def combine_rgb_infra(rgb, infra):\n",
    "    # Concatenates the two modalities along the channels axis\n",
    "    four_channel = np.concatenate((rgb, infra), axis=-1)\n",
    "    return four_channel\n",
    "\n",
    "  \n",
    "def overlapping_patches(images, patch_size=(64, 64), padding=\"VALID\"):\n",
    "    sess = tf.Session()\n",
    "\n",
    "    num_images, size_x, size_y, channels = images.shape\n",
    "    ims = tf.convert_to_tensor(images)\n",
    "    patch_x, patch_y = patch_size\n",
    "    patches = tf.extract_image_patches(ims, [1, patch_x, patch_y, 1], [\n",
    "        1, patch_x, patch_y, 1], [1, 1, 1, 1], padding=padding)\n",
    "    patches_shape = tf.shape(patches)\n",
    "    with sess.as_default():\n",
    "        np = tf.reshape(patches, [tf.reduce_prod(patches_shape[0:3]),\n",
    "                                  patch_x, patch_y, channels]).eval()\n",
    "        return np  \n",
    "\n",
    "\n",
    "def non_overlapping_patches(image, patch_size=(64, 64)):\n",
    "    size_x, size_y, channels = image.shape\n",
    "    patch_x, patch_y = patch_size\n",
    "    im_pad = np.pad(image, ((0, patch_x - size_x % patch_x),\n",
    "                            (0, patch_y - size_y % patch_y), (0, 0)),\n",
    "                    mode=\"constant\")\n",
    "    if size_x % patch_x == 0 and size_y % patch_y == 0:\n",
    "        im_pad = image\n",
    "    pad_x, pad_y, channels = im_pad.shape\n",
    "    print(im_pad.shape)\n",
    "    num_patches = (pad_x // patch_x) * (pad_y // patch_y)\n",
    "    patches = np.zeros((num_patches, patch_x, patch_y, channels))\n",
    "    counter = 0\n",
    "    for i in range((pad_x // patch_x)):\n",
    "        for j in range((pad_y // patch_y)):\n",
    "            x_s = i * patch_x\n",
    "            y_s = j * patch_y\n",
    "            patches[counter, :, :, :] = im_pad[x_s:x_s + patch_x,\n",
    "                                               y_s:y_s + patch_y, :]\n",
    "            counter += 1\n",
    "    return patches\n",
    "\n",
    "\n",
    "def downsample_image(image, block=(4, 4, 1)):\n",
    "    # Downsamples numpy array image by factor\n",
    "    # Returns  the downsampled copy\n",
    "    if image.ndim == 4:\n",
    "      block=(1, 4, 4, 1)\n",
    "    return downscale_local_mean(image, block)\n",
    "\n",
    "\n",
    "def reconstruct_patches(patches, image_size):\n",
    "    # TODO: Create a function which reconstructs an image\n",
    "    # when given patches created by non_overlapping_patches\n",
    "    # Discards predictions for zero border\n",
    "    pass\n",
    "\n",
    "def get_images_to_four_chan(img_name, DATASET_PATH, ch_num=4):\n",
    "  #co = imageio.imread(DATASET_PATH + 'VEDAI/' + img_name + '_co.png')\n",
    "  co = imageio.imread(DATASET_PATH + img_name + '_co.png')\n",
    "  if ch_num == 4:\n",
    "    #ir = imageio.imread(DATASET_PATH + 'VEDAI/' + img_name + '_ir.png')\n",
    "    ir = imageio.imread(DATASET_PATH + img_name + '_ir.png')\n",
    "    rgb = np.reshape(co, (tuple([1]) + co.shape))\n",
    "    infra = np.reshape(ir, (tuple([1]) + ir.shape + tuple([1])))  \n",
    "    return combine_rgb_infra(rgb, infra)\n",
    "  elif ch_num == 3:\n",
    "    return np.reshape(co, (tuple([1]) + co.shape))\n",
    "\n",
    "def load_data(file_idx, txt_file, DATASET_PATH, batch_size=1):\n",
    "  # read in batch of file names from txt file with randomized filenames\n",
    "  # return the lr and hr patches\n",
    "  \n",
    "  #read x lines from txt file\n",
    "  text_file = open(DATASET_PATH +\"/training.txt\", \"r\")\n",
    "  img_files = text_file.read().split('\\n')\n",
    "  text_file.close()\n",
    "\n",
    "# TODO: preallocate arrays for speed  \n",
    "  # Number of patches * ims_per_batch\n",
    "  batchsz = 256 * batch_size\n",
    "  # RGB\n",
    "  channels = 3\n",
    "  # Default patch size\n",
    "  patch_x, patch_y = 64, 64\n",
    "\n",
    "  # Preallocate arrays of the correct size\n",
    "  imgs_hr = np.zeros((batchsz, patch_x, patch_y, channels))\n",
    "  \n",
    "  # Batch number * ims_per_batch\n",
    "  start = file_idx\n",
    "  end = file_idx + batch_size\n",
    "      \n",
    "#   print(start, end)    \n",
    "  \n",
    "  im_num = 0\n",
    "  for i in range(start, end):\n",
    "      st, stp = im_num * 256, (im_num + 1) * 256\n",
    "      im_num += 1 \n",
    "      patch = overlapping_patches(\n",
    "          normalize(get_images_to_four_chan(img_files[i], DATASET_PATH, channels)))\n",
    "      imgs_hr[st:stp, :, :, :] = patch\n",
    "      \n",
    "  imgs_lr = np.asarray([downsample_image(patch) for patch in imgs_hr])\n",
    "      \n",
    "  file_idx = file_idx + batch_size # update current file_idx\n",
    "  return imgs_hr, imgs_lr, file_idx\n",
    "\n",
    "\n",
    "class VEDAISequence(Sequence):\n",
    "\n",
    "    def __init__(self, rgb, infra, ims_per_batch):\n",
    "        self.r, self.i = rgb, infra\n",
    "        self.ims_per_batch = ims_per_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns number of batches given training set and ims_per_batch\n",
    "        return int(np.ceil(len(self.r) / float(self.ims_per_batch)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Number of patches * ims_per_batch\n",
    "        batchsz = 256 * self.ims_per_batch\n",
    "        # RGB and infra\n",
    "        channels = 4\n",
    "        # Default patch size\n",
    "        patch_x, patch_y = 64, 64\n",
    "\n",
    "        # Batch number * ims_per_batch\n",
    "        start = idx * self.ims_per_batch\n",
    "        # Batch number * ims_per_batch  + 1\n",
    "        end = (idx + 1) * self.ims_per_batch\n",
    "\n",
    "        # Preallocate arrays of the correct size\n",
    "        high_res = np.zeros((batchsz, patch_x, patch_y, channels))\n",
    "        im_num = 0\n",
    "        for ind in range(start, end):\n",
    "            st, stp = im_num * 256, (im_num + 1) * 256\n",
    "            im_num += 1\n",
    "            rgb = imageio.imread(self.r[ind])\n",
    "            infra = imageio.imread(self.i[ind])\n",
    "            high_res[st:stp, :, :, :] = non_overlapping_patches(\n",
    "                combine_rgb_infra(rgb, infra))\n",
    "        low_res = np.asarray([downsample_image(patch) for patch in high_res])\n",
    "        return low_res, high_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "uwrcfc4buWYf",
    "outputId": "ae18736f-0dee-428b-b5bf-2b6933141954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following image is missing!: 00000005\n",
      "The following image is missing!: 00000023\n",
      "The following image is missing!: 00000068\n",
      "Using 97 images.\n",
      "Saving files to ../data/Vehicules1024/\n"
     ]
    }
   ],
   "source": [
    "#path = glob.glob('./drive/My Drive/ECE285_Proj/datasets/VEDAI/*')\n",
    "#files = scan_dataset(path, 25)\n",
    "files = scan_dataset(data_dir, 50)\n",
    "#training_set, testing_set = create_subsets(files,'./drive/My Drive/ECE285_Proj/datasets/VEDAI/', use_validation = False)\n",
    "training_set, testing_set = create_subsets(files,data_dir, use_validation = False)\n",
    "\n",
    "# print('Training files: ' + str(training_set))\n",
    "# print('Testing files: ' + str(testing_set))\n",
    "batch_idx = 0\n",
    "#load_test_hr, load_test_lr, batch_idx = load_data(batch_idx, training_set, './drive/My Drive/ECE285_Proj/datasets/VEDAI/', 4)\n",
    "load_test_hr, load_test_lr, batch_idx = load_data(batch_idx, training_set, data_dir, 4)\n",
    "print(\"Test Hi res - \")\n",
    "data_explore(load_test_hr)\n",
    "print(\"Test Lo res - \")\n",
    "data_explore(load_test_lr)\n",
    "print(\"Batch Idx: \" + str(batch_idx))\n",
    "#load_test_hr, load_test_lr, batch_idx = load_data(batch_idx, training_set, './drive/My Drive/ECE285_Proj/datasets/VEDAI/', 4)\n",
    "load_test_hr, load_test_lr, batch_idx = load_data(batch_idx, training_set, data_dir, 4)\n",
    "print(\"Test Hi res - \")\n",
    "data_explore(load_test_hr)\n",
    "print(\"Test Lo res - \")\n",
    "data_explore(load_test_lr)\n",
    "print(\"Batch Idx: \" + str(batch_idx))\n",
    "\n",
    "# rgb, infra = read_VEDAI(training_set, path)\n",
    "# print('RGB shape: '+ str(rgb.shape))\n",
    "# print('Infra shape: '+ str(infra.shape))\n",
    "\n",
    "# training_data = combine_rgb_infra(rgb, infra)\n",
    "# # training_data=rgb\n",
    "# print('Data shape: '+ str(training_data.shape))\n",
    "\n",
    "# training_data = normalize(training_data)\n",
    "\n",
    "# data_patched = overlapping_patches(training_data)\n",
    "# patch_size = data_patched.shape[1]\n",
    "# down_patch_size = int(patch_size/4)\n",
    "# print(\"Patch size \" + str(down_patch_size))\n",
    "\n",
    "# im_hr = np.zeros(data_patched.shape)\n",
    "# im_lr = np.zeros(data_patched.shape)\n",
    "# im_lr = im_lr[:,0:down_patch_size,0:down_patch_size,:]\n",
    "\n",
    "# print('Low res shape: ' + str(im_lr.shape))\n",
    "\n",
    "# factor = 4\n",
    "# patch_size = data_patched.shape[1]\n",
    "# down_patch_size = int(patch_size/factor)\n",
    "\n",
    "\n",
    "# for image in range(data_patched.shape[0]):\n",
    "#   im_lr[image,:,:,:] = downsample_image(data_patched[image,:,:,:])\n",
    "\n",
    "# print(\"Hi res - \")\n",
    "# data_explore(data_patched)\n",
    "# print(\"Lo res - \")\n",
    "# data_explore(im_lr)\n",
    "\n",
    "# plt.figure().suptitle('RGB+Infra', fontsize=20)\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.imshow(data_patched[0,:,:,:])\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.imshow(im_lr[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "colab_type": "code",
    "id": "zD75vf6rLotT",
    "outputId": "a46355fa-5a7b-4fcd-a6a1-2db55d84080b"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SRGAN():\n",
    "    def __init__(self,num_gpus=None, weight_file=None):\n",
    "        # Input shape\n",
    "        self.channels = 3                  # RGB & Infra\n",
    "        self.lr_height = 16                 # Low resolution height\n",
    "        self.lr_width = 16                 # Low resolution width\n",
    "        self.lr_shape = (self.lr_height, self.lr_width, self.channels)\n",
    "        self.hr_height = self.lr_height*4   # High resolution height\n",
    "        self.hr_width = self.lr_width*4     # High resolution width\n",
    "        self.hr_shape = (self.hr_height, self.hr_width, self.channels)\n",
    "        self.num_gpus = num_gpus\n",
    "        self.weight_file = weight_file\n",
    "\n",
    "\n",
    "        # Number of residual blocks in the generator\n",
    "        self.n_residual_blocks = 16\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # We use a pre-trained VGG19 model to extract image features from the high resolution\n",
    "        # and the generated high resolution images and minimize the mse between them\n",
    "        self.vgg = self.build_vgg()\n",
    "        self.vgg.trainable = False\n",
    "        self.vgg.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.hr_height / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 16\n",
    "        self.df = 16\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        \n",
    "        self.discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        \n",
    "        #Configure data loader name, is this still needed?\n",
    "        self.dataset_name = 'VEDAI'\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # High res. and low res. images\n",
    "        img_hr = Input(shape=self.hr_shape)\n",
    "        img_lr = Input(shape=self.lr_shape)\n",
    "\n",
    "        # Generate high res. version from low res.\n",
    "        fake_hr = self.generator(img_lr)\n",
    "\n",
    "        # Extract image features of the generated img\n",
    "        fake_features = self.vgg(fake_hr)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Discriminator determines validity of generated high res. images\n",
    "        validity = self.discriminator(fake_hr)\n",
    "\n",
    "        self.combined = Model([img_lr, img_hr], [validity, fake_features])\n",
    "        self.combined.compile(loss=['binary_crossentropy', 'mse'],\n",
    "                              loss_weights=[1e-3, 1],\n",
    "                              optimizer=optimizer)\n",
    "        \n",
    "        if self.num_gpus is not None:\n",
    "            self.discriminator = multi_gpu_model(self.D, gpus=self.num_gpus)\n",
    "            self.combined = multi_gpu_model(self.G, gpus=self.num_gpus)\n",
    "\n",
    "    def build_vgg(self):\n",
    "        \"\"\"\n",
    "        Builds a pre-trained VGG19 model that outputs image features extracted at the\n",
    "        third block of the model\n",
    "        \"\"\"\n",
    "        vgg = VGG19(weights='imagenet')\n",
    "        # Set outputs to outputs of last conv. layer in block 3\n",
    "        # See architecture at: https://github.com/keras-team/keras/blob/master/keras/applications/vgg19.py\n",
    "        vgg.outputs = [vgg.layers[9].output]\n",
    "\n",
    "        img = Input(shape=self.hr_shape)\n",
    "       \n",
    "        # Extract image features\n",
    "        img_features = vgg(img)\n",
    "\n",
    "        return Model(img, img_features)\n",
    "\n",
    "    def build_generator(self):\n",
    "    \n",
    "        def residual_block(layer_input, filters):\n",
    "            \"\"\"Residual block described in paper\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=3, strides=1, padding='same')(layer_input)\n",
    "            d = Activation('relu')(d)\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Conv2D(filters, kernel_size=3, strides=1, padding='same')(d)\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Add()([d, layer_input])\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(64, kernel_size=3, strides=1, padding='same')(u)\n",
    "            u = Activation('relu')(u)\n",
    "            return u\n",
    "\n",
    "        # Low resolution image input\n",
    "        img_lr = Input(shape=self.lr_shape)\n",
    "\n",
    "        # Pre-residual block\n",
    "        c1 = Conv2D(16, kernel_size=9, strides=1, padding='same')(img_lr)\n",
    "        c1 = Activation('relu')(c1)\n",
    "\n",
    "        # Propogate through residual blocks\n",
    "        r = residual_block(c1, self.gf)\n",
    "        for _ in range(self.n_residual_blocks - 1):\n",
    "            r = residual_block(r, self.gf)\n",
    "\n",
    "        # Post-residual block\n",
    "        c2 = Conv2D(16, kernel_size=3, strides=1, padding='same')(r)\n",
    "        c2 = BatchNormalization(momentum=0.8)(c2)\n",
    "        c2 = Add()([c2, c1])\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(c2)\n",
    "        u2 = deconv2d(u1)\n",
    "\n",
    "        # Generate high resolution output\n",
    "        gen_hr = Conv2D(self.channels, kernel_size=9, strides=1, padding='same', activation='tanh')(u2)\n",
    "        \n",
    "        generator = Model(img_lr, gen_hr)\n",
    "        \n",
    "        if self.weight_file is not None:\n",
    "            generator.load_weights(self.weigths_path)\n",
    "            \n",
    "        return Model(img_lr, gen_hr)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_block(layer_input, filters, strides=1, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=3, strides=strides, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        # Input img\n",
    "        d0 = Input(shape=self.hr_shape)\n",
    "\n",
    "        d1 = d_block(d0, self.df, bn=False)\n",
    "        d2 = d_block(d1, self.df, strides=2)\n",
    "        d3 = d_block(d2, self.df*2)\n",
    "        d4 = d_block(d3, self.df*2, strides=2)\n",
    "        d5 = d_block(d4, self.df*4)\n",
    "        d6 = d_block(d5, self.df*4, strides=2)\n",
    "        d7 = d_block(d6, self.df*8)\n",
    "        d8 = d_block(d7, self.df*8, strides=2)\n",
    "\n",
    "        d9 = Dense(self.df*4)(d8)\n",
    "        d10 = LeakyReLU(alpha=0.2)(d9)\n",
    "        validity = Dense(1, activation='sigmoid')(d10)\n",
    "\n",
    "        return Model(d0, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ----------------------\n",
    "            #  Train Discriminator\n",
    "            # ----------------------\n",
    "\n",
    "            # Sample images and their conditioning counterparts\n",
    "            batch_idx = 0\n",
    "            imgs_hr, imgs_lr, batch_idx = load_data(batch_idx, training_set, data_dir, batch_size)\n",
    "\n",
    "            # From low res. image generate high res. version\n",
    "            fake_hr = self.generator.predict(imgs_lr)\n",
    "\n",
    "            valid = np.ones((imgs_hr.shape[0],) + self.disc_patch)\n",
    "            fake = np.zeros((imgs_hr.shape[0],) + self.disc_patch)\n",
    "\n",
    "            # Train the discriminators (original images = real / generated = Fake)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs_hr, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(fake_hr, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generator\n",
    "            # ------------------\n",
    "\n",
    "            # Sample images and their conditioning counterparts\n",
    "            imgs_hr, imgs_lr, batch_idx = load_data(batch_idx, training_set,\n",
    "                      data_dir,\n",
    "                      batch_size)\n",
    "    \n",
    "            # The generators want the discriminators to label the generated images as real\n",
    "            valid = np.ones((imgs_hr.shape[0],) + self.disc_patch)\n",
    "\n",
    "            # Extract ground truth image features using pre-trained VGG19 model\n",
    "            image_features = self.vgg.predict(imgs_hr)\n",
    "\n",
    "            # Train the generators\n",
    "            g_loss = self.combined.train_on_batch([imgs_lr, imgs_hr], [valid, image_features])\n",
    "\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "            # Plot the progress\n",
    "            print (\"%d time: %s\" % (epoch, elapsed_time))\n",
    "            print(\"\\tG-Loss: \" + str(g_loss) + \"\\tD-Loss: \" + str(d_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "                self.generator.save('./OvrSR_generator_weights.h5')\n",
    "                self.discriminator.save('./OvrSR_discriminator_weights.h5')\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        os.makedirs('./drive/My Drive/ECE285_Proj/images/%s' % self.dataset_name, exist_ok=True)\n",
    "        r, c = 2, 2\n",
    "        \n",
    "        batch_idx = 0\n",
    "        #imgs_hr, imgs_lr, batch_idx = load_data(batch_idx, testing_set, './drive/My Drive/ECE285_Proj/datasets/VEDAI/')\n",
    "        imgs_hr, imgs_lr, batch_idx = load_data(batch_idx, testing_set, data_dir)\n",
    "        fake_hr = self.generator.predict(imgs_lr)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        imgs_lr = 0.5 * imgs_lr + 0.5\n",
    "        fake_hr = 0.5 * fake_hr + 0.5\n",
    "        imgs_hr = 0.5 * imgs_hr + 0.5\n",
    "\n",
    "        # Save generated images and the high resolution originals\n",
    "        titles = ['Generated', 'Original']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        fig.suptitle('Epoch %d' % (epoch), fontsize=16)\n",
    "        for row in range(r):\n",
    "            for col, image in enumerate([fake_hr, imgs_hr]):\n",
    "                axs[row, col].imshow(image[row])\n",
    "                axs[row, col].set_title(titles[col])\n",
    "                axs[row, col].axis('off')\n",
    "            cnt += 1\n",
    "        #fig.savefig(\"./drive/My Drive/ECE285_Proj/images/%s%d.png\" % (self.dataset_name, epoch))\n",
    "        fig.savefig(dir_pfx + 'images/%s%d.png'.format(self.dataset_name, epoch))\n",
    "        plt.close()\n",
    "\n",
    "        # Save low resolution images for comparison\n",
    "        for i in range(r):\n",
    "            fig = plt.figure()\n",
    "            plt.imshow(imgs_lr[i])\n",
    "            #fig.savefig('./drive/My Drive/ECE285_Proj/images/%s/%d_lowres%d.png' % (self.dataset_name, epoch, i))\n",
    "            fig.savefig(dir_pfx + 'images/%s/%d_lowres%d.png'.format(self.dataset_name, epoch, i))\n",
    "            plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Init model...\")\n",
    "    gan = SRGAN(num_gpus=8, weight_file='./OvrSR_generator_weights.h5')\n",
    "    print(\"Starting training...\")\n",
    "    gan.train(epochs=100, batch_size=4, sample_interval=5)\n",
    "#     gan.train(epochs=300, batch_size=1, sample_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G4GP-NAT1WvA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Keras OvrSR_V2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
