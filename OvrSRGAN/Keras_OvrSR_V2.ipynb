{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mihirsathe/ECE285FA18_BestGroup/blob/master/Keras_OvrSR_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "id": "etiSuCAVNfhL",
    "outputId": "8fb95154-1820-4416-8892-5d1cf8ff4113"
   },
   "outputs": [],
   "source": [
    "#!pip3 install -q git+https://www.github.com/keras-team/keras-contrib.git\n",
    "#!pip3 install -q DataLoader\n",
    "#!pip3 install -q imageio\n",
    "#!pip3 install -q keras\n",
    "\n",
    "#Mount google drive with dataset\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "dir_pfx = './'\n",
    "data_dir = '../../data/Vehicules1024/'\n",
    "# keras.backend.set_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xaMsQct9Lxem",
    "outputId": "2857e543-c53c-4e80-858f-73e41622191e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Super-resolution of CelebA using Generative Adversarial Networks.\n",
    "# The dataset can be downloaded from: https://www.dropbox.com/sh/8oqt9vytwxb3s4r/AADIKlz8PR9zr6Y20qbkunrba/Img/img_align_celeba.zip?dl=0\n",
    "# Instrustion on running the script:\n",
    "# 1. Download the dataset from the provided link\n",
    "# 2. Save the folder 'img_align_celeba' to 'datasets/'\n",
    "# 4. Run the sript using command 'python srgan.py'\n",
    "\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import scipy\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Add\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.applications import VGG19\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# from google.col\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import keras.backend as K\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M6s0jXbDLqgV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "from keras.utils import Sequence\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from skimage.transform import downscale_local_mean\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "  return image/255.0    \n",
    "\n",
    "def read_VEDAI(subset, PATH_TO_VEHICLES_FOLDER):\n",
    "    # Takes in the full path to the unzipped \"VEHICULES\" folder\n",
    "    # Returns mapping dict, RGB and Infrared images in \n",
    "    # (images, x, y, channels) format,\n",
    "    # saves a txt file with mapping of rgb/infra idx to filename\n",
    "    # NUM_FILES = 2536\n",
    "    # MAX_INDEX = 1272\n",
    "    X_PIXELS = 1024\n",
    "    Y_PIXELS = 1024\n",
    "    PATH_TO_VEHICLES_FOLDER = PATH_TO_VEHICLES_FOLDER#[0]\n",
    "\n",
    "    onlyfiles = [f for f in listdir(PATH_TO_VEHICLES_FOLDER) if isfile(\n",
    "        join(PATH_TO_VEHICLES_FOLDER, f)) and \"png\" in f]\n",
    "    # assert len(onlyfiles) == NUM_FILES, \"Not the full VEDAI 1024 Dataset\"\n",
    "    rgb = np.zeros((len(subset), X_PIXELS, Y_PIXELS, 3))\n",
    "    infra = np.zeros((len(subset), X_PIXELS, Y_PIXELS, 1))\n",
    "    indices = subset\n",
    "\n",
    "    print(indices)\n",
    "    print(rgb.shape)\n",
    "    index_filename_map = {}\n",
    "    im_cnt = 0\n",
    "\n",
    "    for index in indices:\n",
    "        pair = [file for file in onlyfiles if str(index) in file]\n",
    "        if pair:\n",
    "            for file in pair:\n",
    "                index_filename_map[im_cnt] = file.split('_')[0]\n",
    "                im = imageio.imread(PATH_TO_VEHICLES_FOLDER + '/' + file)\n",
    "                if \"co\" in file:\n",
    "                    # print('Inserting RGB @ '+ str(im_cnt))\n",
    "                    rgb[im_cnt, :, :, :] = np.reshape(\n",
    "                        im, (tuple([1]) + im.shape))\n",
    "                elif \"ir\" in file:\n",
    "                    # print('Inserting Infra @ '+ str(im_cnt))\n",
    "                    infra[im_cnt, :, :, :] = np.reshape(\n",
    "                        im, (tuple([1]) + im.shape + tuple([1])))\n",
    "            im_cnt = im_cnt + 1\n",
    "        else:\n",
    "            print(\"The following image is missing!: \" + index)\n",
    "    # print(index_filename_map)\n",
    "    f = open(PATH_TO_VEHICLES_FOLDER + \"_mapping.txt\", \"w\")\n",
    "    f.write(str(index_filename_map))\n",
    "    f.close()\n",
    "    return rgb, infra\n",
    "\n",
    "\n",
    "def scan_dataset(PATH_TO_VEHICLES_FOLDER, number_of_imgs):\n",
    "    # Takes in the full path to the unzipped \"VEHICULES\" folder\n",
    "    # Returns a list of all the files\n",
    "    # and saves a dataset summary text file with list of all file names\n",
    "    MAX_INDEX = number_of_imgs\n",
    "    PATH_TO_VEHICLES_FOLDER = PATH_TO_VEHICLES_FOLDER#[0]\n",
    "    indices = [format(n, '08') for n in range(MAX_INDEX)]\n",
    "    export_files = []\n",
    "\n",
    "    onlyfiles = [f for f in listdir(PATH_TO_VEHICLES_FOLDER) if isfile(\n",
    "        join(PATH_TO_VEHICLES_FOLDER, f)) and \"png\" in f]\n",
    "\n",
    "    for index in indices:\n",
    "        pair = [file for file in onlyfiles if str(index) in file]\n",
    "        if pair:\n",
    "            for file in pair:\n",
    "                if \"co\" in file:\n",
    "                    export_files.append(file.split('_')[0])\n",
    "        else:\n",
    "            print(\"The following image is missing!: \" + index)\n",
    "    np.savetxt(PATH_TO_VEHICLES_FOLDER + '_summary.txt',\n",
    "               export_files, delimiter=\" \", fmt=\"%s\")\n",
    "    return export_files\n",
    "\n",
    "\n",
    "def create_subsets(imgs, output_path, use_validation=True,\n",
    "                   training_percent=0.7, testing_percent=0.3, SEED=1):\n",
    "    # Takes a list of image file names and shuffles\n",
    "    # them before splitting them into required subsets\n",
    "    # Saves txt files containing the names of the files\n",
    "    # used in each subset, no return value\n",
    "\n",
    "    assert training_percent + \\\n",
    "        testing_percent == 1, \"Training + testing percents must equal 1.\"\n",
    "    random.seed(SEED)\n",
    "    random.shuffle(imgs)\n",
    "    print('Using ' + str(len(imgs)) + ' images.')\n",
    "    print('Saving files to ' + output_path)\n",
    "    if not use_validation:\n",
    "        training_imgs = imgs[:int(len(imgs) * training_percent)]\n",
    "        testing_imgs = imgs[int(len(imgs) * training_percent):]\n",
    "        np.savetxt(output_path + 'training.txt',\n",
    "                   training_imgs, delimiter=\" \", fmt=\"%s\")\n",
    "        np.savetxt(output_path + 'testing.txt',\n",
    "                   testing_imgs, delimiter=\" \", fmt=\"%s\")\n",
    "        return training_imgs, testing_imgs\n",
    "    else:\n",
    "        validation_split = 0.3  # use 30% of training dataset for validation\n",
    "        training_imgs = imgs[int(len(imgs) * validation_split *\n",
    "                                 training_percent):int(len(imgs) *\n",
    "                                                       training_percent)]\n",
    "        validation_imgs = imgs[:int(\n",
    "            len(imgs) * validation_split * training_percent)]\n",
    "        testing_imgs = imgs[int(len(imgs) * training_percent):]\n",
    "        np.savetxt(output_path + 'validation.txt',\n",
    "                   validation_imgs, delimiter=\" \", fmt=\"%s\")\n",
    "        np.savetxt(output_path + 'training.txt',\n",
    "                   testing_imgs, delimiter=\" \", fmt=\"%s\")\n",
    "        np.savetxt(output_path + 'testing.txt',\n",
    "                   testing_imgs, delimiter=\" \", fmt=\"%s\")\n",
    "        return training_imgs, validation_imgs, testing_imgs\n",
    "\n",
    "\n",
    "def save_VEDAI(rgb, infra):\n",
    "    # Takes in arrays of rgb and infrared images\n",
    "    # Saves them to disk, no return value\n",
    "    np.save(\"vedai_rgb_all.npy\", rgb)\n",
    "    np.save(\"vedai_infra_all.npy\", infra)\n",
    "\n",
    "\n",
    "def load_VEDAI():\n",
    "    # No parameters, expected to run in directory with VEDAI.npy files\n",
    "    # Returns two arrays with rgb and infrared images respectively\n",
    "    rgb = np.load(\"vedai_rgb_all.npy\")\n",
    "    infra = np.load(\"vedai_infra_all.npy\")\n",
    "    return rgb, infra\n",
    "\n",
    "\n",
    "def data_explore(data):\n",
    "    print(\"Shape of the data is\" + str(data.shape))\n",
    "    print(\"Dtype of the data is\" + str(data.dtype))\n",
    "\n",
    "\n",
    "def combine_rgb_infra(rgb, infra):\n",
    "    # Concatenates the two modalities along the channels axis\n",
    "    four_channel = np.concatenate((rgb, infra), axis=-1)\n",
    "    return four_channel\n",
    "\n",
    "  \n",
    "def overlapping_patches(images, patch_size=(64, 64), padding=\"VALID\"):\n",
    "    sess = tf.Session()\n",
    "\n",
    "    num_images, size_x, size_y, channels = images.shape\n",
    "    ims = tf.convert_to_tensor(images)\n",
    "    patch_x, patch_y = patch_size\n",
    "    patches = tf.extract_image_patches(ims, [1, patch_x, patch_y, 1], [\n",
    "        1, patch_x, patch_y, 1], [1, 1, 1, 1], padding=padding)\n",
    "    patches_shape = tf.shape(patches)\n",
    "    with sess.as_default():\n",
    "        np = tf.reshape(patches, [tf.reduce_prod(patches_shape[0:3]),\n",
    "                                  patch_x, patch_y, channels]).eval()\n",
    "        return np  \n",
    "\n",
    "\n",
    "def non_overlapping_patches(image, patch_size=(64, 64)):\n",
    "    size_x, size_y, channels = image.shape\n",
    "    patch_x, patch_y = patch_size\n",
    "    im_pad = np.pad(image, ((0, patch_x - size_x % patch_x),\n",
    "                            (0, patch_y - size_y % patch_y), (0, 0)),\n",
    "                    mode=\"constant\")\n",
    "    if size_x % patch_x == 0 and size_y % patch_y == 0:\n",
    "        im_pad = image\n",
    "    pad_x, pad_y, channels = im_pad.shape\n",
    "    print(im_pad.shape)\n",
    "    num_patches = (pad_x // patch_x) * (pad_y // patch_y)\n",
    "    patches = np.zeros((num_patches, patch_x, patch_y, channels))\n",
    "    counter = 0\n",
    "    for i in range((pad_x // patch_x)):\n",
    "        for j in range((pad_y // patch_y)):\n",
    "            x_s = i * patch_x\n",
    "            y_s = j * patch_y\n",
    "            patches[counter, :, :, :] = im_pad[x_s:x_s + patch_x,\n",
    "                                               y_s:y_s + patch_y, :]\n",
    "            counter += 1\n",
    "    return patches\n",
    "\n",
    "\n",
    "def downsample_image(image, block=(4, 4, 1)):\n",
    "    # Downsamples numpy array image by factor\n",
    "    # Returns  the downsampled copy\n",
    "    if image.ndim == 4:\n",
    "      block=(1, 4, 4, 1)\n",
    "    return downscale_local_mean(image, block)\n",
    "\n",
    "\n",
    "def reconstruct_patches(patches, image_size):\n",
    "    # TODO: Create a function which reconstructs an image\n",
    "    # when given patches created by non_overlapping_patches\n",
    "    # Discards predictions for zero border\n",
    "    pass\n",
    "\n",
    "def get_images_to_four_chan(img_name, DATASET_PATH, ch_num=4):\n",
    "  #co = imageio.imread(DATASET_PATH + 'VEDAI/' + img_name + '_co.png')\n",
    "  co = imageio.imread(DATASET_PATH + img_name + '_co.png')\n",
    "  if ch_num == 4:\n",
    "    #ir = imageio.imread(DATASET_PATH + 'VEDAI/' + img_name + '_ir.png')\n",
    "    ir = imageio.imread(DATASET_PATH + img_name + '_ir.png')\n",
    "    rgb = np.reshape(co, (tuple([1]) + co.shape))\n",
    "    infra = np.reshape(ir, (tuple([1]) + ir.shape + tuple([1])))  \n",
    "    return combine_rgb_infra(rgb, infra)\n",
    "  elif ch_num == 3:\n",
    "    return np.reshape(co, (tuple([1]) + co.shape))\n",
    "\n",
    "def load_data(file_idx, txt_file, DATASET_PATH, batch_size=1):\n",
    "  # read in batch of file names from txt file with randomized filenames\n",
    "  # return the lr and hr patches\n",
    "  \n",
    "  #read x lines from txt file\n",
    "  text_file = open(DATASET_PATH +\"/training.txt\", \"r\")\n",
    "  img_files = text_file.read().split('\\n')\n",
    "  text_file.close()\n",
    "\n",
    "# TODO: preallocate arrays for speed  \n",
    "  # Number of patches * ims_per_batch\n",
    "  batchsz = 256 * batch_size\n",
    "  # RGB\n",
    "  channels = 3\n",
    "  # Default patch size\n",
    "  patch_x, patch_y = 64, 64\n",
    "\n",
    "  # Preallocate arrays of the correct size\n",
    "  imgs_hr = np.zeros((batchsz, patch_x, patch_y, channels))\n",
    "  \n",
    "  # Batch number * ims_per_batch\n",
    "  start = file_idx\n",
    "  end = file_idx + batch_size\n",
    "      \n",
    "#   print(start, end)    \n",
    "  \n",
    "  im_num = 0\n",
    "  for i in range(start, end):\n",
    "      st, stp = im_num * 256, (im_num + 1) * 256\n",
    "      im_num += 1 \n",
    "      patch = overlapping_patches(\n",
    "          normalize(get_images_to_four_chan(img_files[i], DATASET_PATH, channels)))\n",
    "      imgs_hr[st:stp, :, :, :] = patch\n",
    "      \n",
    "  imgs_lr = np.asarray([downsample_image(patch) for patch in imgs_hr])\n",
    "      \n",
    "  file_idx = file_idx + batch_size # update current file_idx\n",
    "  return imgs_hr, imgs_lr, file_idx\n",
    "\n",
    "\n",
    "class VEDAISequence(Sequence):\n",
    "\n",
    "    def __init__(self, rgb, infra, ims_per_batch):\n",
    "        self.r, self.i = rgb, infra\n",
    "        self.ims_per_batch = ims_per_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns number of batches given training set and ims_per_batch\n",
    "        return int(np.ceil(len(self.r) / float(self.ims_per_batch)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Number of patches * ims_per_batch\n",
    "        batchsz = 256 * self.ims_per_batch\n",
    "        # RGB and infra\n",
    "        channels = 4\n",
    "        # Default patch size\n",
    "        patch_x, patch_y = 64, 64\n",
    "\n",
    "        # Batch number * ims_per_batch\n",
    "        start = idx * self.ims_per_batch\n",
    "        # Batch number * ims_per_batch  + 1\n",
    "        end = (idx + 1) * self.ims_per_batch\n",
    "\n",
    "        # Preallocate arrays of the correct size\n",
    "        high_res = np.zeros((batchsz, patch_x, patch_y, channels))\n",
    "        im_num = 0\n",
    "        for ind in range(start, end):\n",
    "            st, stp = im_num * 256, (im_num + 1) * 256\n",
    "            im_num += 1\n",
    "            rgb = imageio.imread(self.r[ind])\n",
    "            infra = imageio.imread(self.i[ind])\n",
    "            high_res[st:stp, :, :, :] = non_overlapping_patches(\n",
    "                combine_rgb_infra(rgb, infra))\n",
    "        low_res = np.asarray([downsample_image(patch) for patch in high_res])\n",
    "        return low_res, high_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "uwrcfc4buWYf",
    "outputId": "ae18736f-0dee-428b-b5bf-2b6933141954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following image is missing!: 00000005\n",
      "The following image is missing!: 00000023\n",
      "Using 48 images.\n",
      "Saving files to ../data/Vehicules1024/\n"
     ]
    }
   ],
   "source": [
    "#path = glob.glob('./drive/My Drive/ECE285_Proj/datasets/VEDAI/*')\n",
    "#files = scan_dataset(path, 25)\n",
    "files = scan_dataset(data_dir, 50) #TODO: 50\n",
    "#training_set, testing_set = create_subsets(files,'./drive/My Drive/ECE285_Proj/datasets/VEDAI/', use_validation = False)\n",
    "training_set, testing_set = create_subsets(files,data_dir, use_validation = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "colab_type": "code",
    "id": "zD75vf6rLotT",
    "outputId": "a46355fa-5a7b-4fcd-a6a1-2db55d84080b"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SRGAN():\n",
    "    def __init__(self,num_gpus=None, weights_path=None):\n",
    "        # Input shape\n",
    "        self.channels = 3                  # RGB & Infra\n",
    "        self.lr_height = 16                 # Low resolution height\n",
    "        self.lr_width = 16                 # Low resolution width\n",
    "        self.lr_shape = (self.lr_height, self.lr_width, self.channels)\n",
    "        self.hr_height = self.lr_height*4   # High resolution height\n",
    "        self.hr_width = self.lr_width*4     # High resolution width\n",
    "        self.hr_shape = (self.hr_height, self.hr_width, self.channels)\n",
    "        self.num_gpus = num_gpus\n",
    "        self.weights_path = weights_path\n",
    "\n",
    "\n",
    "        # Number of residual blocks in the generator\n",
    "        self.n_residual_blocks = 16\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # We use a pre-trained VGG19 model to extract image features from the high resolution\n",
    "        # and the generated high resolution images and minimize the mse between them\n",
    "        self.vgg = self.build_vgg()\n",
    "        self.vgg.trainable = False\n",
    "        self.vgg.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.hr_height / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 16\n",
    "        self.df = 16\n",
    "\n",
    "        # Build and compile the discriminator (Keep off GPU)\n",
    "        self.disc_model = self.build_discriminator()\n",
    "        \n",
    "        if self.num_gpus is not None:\n",
    "            self.discriminator = multi_gpu_model(self.disc_model, gpus=self.num_gpus)\n",
    "        \n",
    "        self.discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        \n",
    "        #Configure data loader name, is this still needed?\n",
    "        self.dataset_name = 'VEDAI'\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # High res. and low res. images\n",
    "        img_hr = Input(shape=self.hr_shape)\n",
    "        img_lr = Input(shape=self.lr_shape)\n",
    "\n",
    "        # Generate high res. version from low res.\n",
    "        fake_hr = self.generator(img_lr)\n",
    "\n",
    "        # Extract image features of the generated img\n",
    "        fake_features = self.vgg(fake_hr)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Discriminator determines validity of generated high res. images\n",
    "        validity = self.discriminator(fake_hr)\n",
    "\n",
    "        self.combined = Model([img_lr, img_hr], [validity, fake_features])\n",
    "        \n",
    "        if self.num_gpus is not None:\n",
    "            self.combined = multi_gpu_model(self.combined, gpus=self.num_gpus)       \n",
    "        \n",
    "        self.combined.compile(loss=['binary_crossentropy', 'mse'],\n",
    "                              loss_weights=[1e-3, 1],\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "    def build_vgg(self):\n",
    "        \"\"\"\n",
    "        Builds a pre-trained VGG19 model that outputs image features extracted at the\n",
    "        third block of the model\n",
    "        \"\"\"\n",
    "        vgg = VGG19(weights='imagenet')\n",
    "        # Set outputs to outputs of last conv. layer in block 3\n",
    "        # See architecture at: https://github.com/keras-team/keras/blob/master/keras/applications/vgg19.py\n",
    "        vgg.outputs = [vgg.layers[9].output]\n",
    "\n",
    "        img = Input(shape=self.hr_shape)\n",
    "       \n",
    "        # Extract image features\n",
    "        img_features = vgg(img)\n",
    "\n",
    "        return Model(img, img_features)\n",
    "\n",
    "    def build_generator(self):\n",
    "    \n",
    "        def residual_block(layer_input, filters):\n",
    "            \"\"\"Residual block described in paper\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=3, strides=1, padding='same')(layer_input)\n",
    "            d = Activation('relu')(d)\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Conv2D(filters, kernel_size=3, strides=1, padding='same')(d)\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Add()([d, layer_input])\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(64, kernel_size=3, strides=1, padding='same')(u)\n",
    "            u = Activation('relu')(u)\n",
    "            return u\n",
    "\n",
    "        # Low resolution image input\n",
    "        img_lr = Input(shape=self.lr_shape)\n",
    "\n",
    "        # Pre-residual block\n",
    "        c1 = Conv2D(16, kernel_size=9, strides=1, padding='same')(img_lr)\n",
    "        c1 = Activation('relu')(c1)\n",
    "\n",
    "        # Propogate through residual blocks\n",
    "        r = residual_block(c1, self.gf)\n",
    "        for _ in range(self.n_residual_blocks - 1):\n",
    "            r = residual_block(r, self.gf)\n",
    "\n",
    "        # Post-residual block\n",
    "        c2 = Conv2D(16, kernel_size=3, strides=1, padding='same')(r)\n",
    "        c2 = BatchNormalization(momentum=0.8)(c2)\n",
    "        c2 = Add()([c2, c1])\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(c2)\n",
    "        u2 = deconv2d(u1)\n",
    "\n",
    "        # Generate high resolution output\n",
    "        gen_hr = Conv2D(self.channels, kernel_size=9, strides=1, padding='same', activation='tanh')(u2)\n",
    "        \n",
    "        generator = Model(img_lr, gen_hr)\n",
    "        \n",
    "        if self.weights_path is not None:\n",
    "            print('Loading generator weights...')\n",
    "            generator.load_weights(self.weights_path + 'OvrSR_generator_weights.h5')\n",
    "            print('Loaded...')\n",
    "            \n",
    "        return Model(img_lr, gen_hr)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_block(layer_input, filters, strides=1, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=3, strides=strides, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        # Input img\n",
    "        d0 = Input(shape=self.hr_shape)\n",
    "\n",
    "        d1 = d_block(d0, self.df, bn=False)\n",
    "        d2 = d_block(d1, self.df, strides=2)\n",
    "        d3 = d_block(d2, self.df*2)\n",
    "        d4 = d_block(d3, self.df*2, strides=2)\n",
    "        d5 = d_block(d4, self.df*4)\n",
    "        d6 = d_block(d5, self.df*4, strides=2)\n",
    "        d7 = d_block(d6, self.df*8)\n",
    "        d8 = d_block(d7, self.df*8, strides=2)\n",
    "\n",
    "        d9 = Dense(self.df*4)(d8)\n",
    "        d10 = LeakyReLU(alpha=0.2)(d9)\n",
    "        validity = Dense(1, activation='sigmoid')(d10)\n",
    "\n",
    "        return Model(d0, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ----------------------\n",
    "            #  Train Discriminator\n",
    "            # ----------------------\n",
    "\n",
    "            # Sample images and their conditioning counterparts\n",
    "            batch_idx = 0\n",
    "            imgs_hr, imgs_lr, batch_idx = load_data(batch_idx, training_set, data_dir, batch_size)\n",
    "\n",
    "            # From low res. image generate high res. version\n",
    "            fake_hr = self.generator.predict(imgs_lr)\n",
    "\n",
    "            valid = np.ones((imgs_hr.shape[0],) + self.disc_patch)\n",
    "            fake = np.zeros((imgs_hr.shape[0],) + self.disc_patch)\n",
    "\n",
    "            # Train the discriminators (original images = real / generated = Fake)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs_hr, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(fake_hr, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generator\n",
    "            # ------------------\n",
    "\n",
    "            # Sample images and their conditioning counterparts\n",
    "            imgs_hr, imgs_lr, batch_idx = load_data(batch_idx, training_set,\n",
    "                      data_dir,\n",
    "                      batch_size)\n",
    "    \n",
    "            # The generators want the discriminators to label the generated images as real\n",
    "            valid = np.ones((imgs_hr.shape[0],) + self.disc_patch)\n",
    "\n",
    "            # Extract ground truth image features using pre-trained VGG19 model\n",
    "            image_features = self.vgg.predict(imgs_hr)\n",
    "\n",
    "            # Train the generators\n",
    "            g_loss = self.combined.train_on_batch([imgs_lr, imgs_hr], [valid, image_features])\n",
    "\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "            # Plot the progress\n",
    "            print (\"%d time: %s\" % (epoch, elapsed_time))\n",
    "            print(\"\\tG-Loss: \" + str(g_loss) + \"\\tD-Loss: \" + str(d_loss))\n",
    "            \n",
    "            # Append current losses and save\n",
    "            \n",
    "            losses.append([epoch] + list(g_loss) + list(d_loss))\n",
    "            \n",
    "            np.save(dir_pfx + 'loss_log.npy', arr=np.array(losses))            \n",
    "            \n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "                self.generator.save(self.weights_path + 'OvrSR_generator_weights.h5')\n",
    "                self.disc_model.save(self.weights_path + 'OvrSR_discriminator_weights.h5')\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        os.makedirs(dir_pfx + 'images/{0}'.format(self.dataset_name), exist_ok=True)\n",
    "        r, c = 2, 2\n",
    "        \n",
    "        batch_idx = 0\n",
    "        #imgs_hr, imgs_lr, batch_idx = load_data(batch_idx, testing_set, './drive/My Drive/ECE285_Proj/datasets/VEDAI/')\n",
    "        imgs_hr, imgs_lr, batch_idx = load_data(batch_idx, testing_set, data_dir)\n",
    "        fake_hr = self.generator.predict(imgs_lr)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        imgs_lr = 0.5 * imgs_lr + 0.5\n",
    "        fake_hr = 0.5 * fake_hr + 0.5\n",
    "        imgs_hr = 0.5 * imgs_hr + 0.5\n",
    "\n",
    "        # Save generated images and the high resolution originals\n",
    "        titles = ['Generated', 'Original']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        fig.suptitle('Epoch %d' % (epoch), fontsize=16)\n",
    "        for row in range(r):\n",
    "            for col, image in enumerate([fake_hr, imgs_hr]):\n",
    "                axs[row, col].imshow(image[row])\n",
    "                axs[row, col].set_title(titles[col])\n",
    "                axs[row, col].axis('off')\n",
    "            cnt += 1\n",
    "        #fig.savefig(\"./drive/My Drive/ECE285_Proj/images/%s%d.png\" % (self.dataset_name, epoch))\n",
    "        fig.savefig(dir_pfx + 'images/{0}{1}.png'.format(self.dataset_name, epoch))\n",
    "        plt.close()\n",
    "\n",
    "        # Save low resolution images for comparison\n",
    "        for i in range(r):\n",
    "            fig = plt.figure()\n",
    "            plt.imshow(imgs_lr[i])\n",
    "            #fig.savefig('./drive/My Drive/ECE285_Proj/images/%s/%d_lowres%d.png' % (self.dataset_name, epoch, i))\n",
    "            fig.savefig(dir_pfx + 'images/{0}/{1}_lowres{2}.png'.format(self.dataset_name, epoch, i))\n",
    "            plt.close()\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    print(\"Init model...\")\n",
    "#    gan = SRGAN(num_gpus=8, weights_path='./')\n",
    "#    print(\"Starting training...\")\n",
    "#    gan.train(epochs=1000, batch_size=4, sample_interval=5)\n",
    "#     gan.train(epochs=300, batch_size=1, sample_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init model...\n",
      "Loading generator weights...\n",
      "Loaded...\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 time: 0:04:44.808922\n",
      "\tG-Loss: [13.9393635, 0.84455097, 13.938519]\tD-Loss: [0.2979524  0.47912598]\n",
      "1 time: 0:10:27.511441\n",
      "\tG-Loss: [13.939474, 0.9394254, 13.938535]\tD-Loss: [0.25528893 0.5744629 ]\n",
      "2 time: 0:12:46.689204\n",
      "\tG-Loss: [14.015808, 1.0837474, 14.014725]\tD-Loss: [0.2147986 0.684906 ]\n",
      "3 time: 0:15:11.493832\n",
      "\tG-Loss: [13.349136, 1.289816, 13.347847]\tD-Loss: [0.17503083 0.7739563 ]\n",
      "4 time: 0:17:42.867887\n",
      "\tG-Loss: [13.252498, 1.4348549, 13.251062]\tD-Loss: [0.13868488 0.85324097]\n",
      "5 time: 0:20:12.317777\n",
      "\tG-Loss: [12.973996, 1.6276083, 12.972368]\tD-Loss: [0.10956345 0.90252686]\n",
      "6 time: 0:23:23.961883\n",
      "\tG-Loss: [12.754023, 1.756727, 12.752266]\tD-Loss: [0.08253214 0.94763184]\n",
      "7 time: 0:26:03.019418\n",
      "\tG-Loss: [12.620253, 1.874224, 12.618379]\tD-Loss: [0.06503311 0.96676636]\n",
      "8 time: 0:28:46.471903\n",
      "\tG-Loss: [12.485141, 2.0052433, 12.483135]\tD-Loss: [0.04993654 0.98257446]\n",
      "9 time: 0:31:32.836518\n",
      "\tG-Loss: [12.371553, 2.0914702, 12.369462]\tD-Loss: [0.04060389 0.9878845 ]\n",
      "10 time: 0:34:25.675448\n",
      "\tG-Loss: [12.266894, 2.1782868, 12.264716]\tD-Loss: [0.0327863  0.99209595]\n",
      "11 time: 0:38:01.146846\n",
      "\tG-Loss: [12.180763, 2.2638571, 12.178499]\tD-Loss: [0.02757154 0.99453735]\n",
      "12 time: 0:40:58.847070\n",
      "\tG-Loss: [12.117659, 2.3179078, 12.11534]\tD-Loss: [0.02406712 0.9959717 ]\n",
      "13 time: 0:44:07.353403\n",
      "\tG-Loss: [12.167714, 2.3853228, 12.165329]\tD-Loss: [0.02133022 0.99694824]\n",
      "14 time: 0:47:18.879667\n",
      "\tG-Loss: [12.371754, 2.3995757, 12.369354]\tD-Loss: [0.01987404 0.9970398 ]\n",
      "15 time: 0:50:33.455991\n",
      "\tG-Loss: [13.19175, 2.432382, 13.189317]\tD-Loss: [0.01941198 0.9975891 ]\n",
      "16 time: 0:54:30.722331\n",
      "\tG-Loss: [12.829178, 2.501227, 12.826676]\tD-Loss: [0.01740212 0.9976196 ]\n",
      "17 time: 0:57:49.154693\n",
      "\tG-Loss: [12.404405, 2.5572042, 12.401848]\tD-Loss: [0.01505246 0.9987793 ]\n",
      "18 time: 1:01:11.368260\n",
      "\tG-Loss: [12.240066, 2.6452956, 12.23742]\tD-Loss: [0.01243629 0.99905396]\n",
      "19 time: 1:04:44.350690\n",
      "\tG-Loss: [11.923001, 2.6788056, 11.920322]\tD-Loss: [0.01176978 0.9989319 ]\n",
      "20 time: 1:08:21.659973\n",
      "\tG-Loss: [11.764229, 2.7433088, 11.761485]\tD-Loss: [0.01037624 0.99935913]\n",
      "21 time: 1:12:42.046680\n",
      "\tG-Loss: [11.615555, 2.7810454, 11.612774]\tD-Loss: [0.00985373 0.99935913]\n",
      "22 time: 1:16:22.164704\n",
      "\tG-Loss: [11.505998, 2.8076293, 11.50319]\tD-Loss: [0.00919054 0.99954224]\n",
      "23 time: 1:20:13.487578\n",
      "\tG-Loss: [11.4101925, 2.8251226, 11.407368]\tD-Loss: [0.00862619 0.9996643 ]\n",
      "24 time: 1:24:07.419357\n",
      "\tG-Loss: [11.328909, 2.8547816, 11.326055]\tD-Loss: [0.00852456 0.9996338 ]\n",
      "25 time: 1:28:04.897664\n",
      "\tG-Loss: [11.251281, 2.8673203, 11.248413]\tD-Loss: [0.00801256 0.9996643 ]\n",
      "26 time: 1:32:44.186621\n",
      "\tG-Loss: [11.182323, 2.8955681, 11.179428]\tD-Loss: [0.00779922 0.9996948 ]\n",
      "27 time: 1:36:38.932570\n",
      "\tG-Loss: [11.132251, 2.8985016, 11.129353]\tD-Loss: [0.00751873 0.99975586]\n",
      "28 time: 1:40:37.043657\n",
      "\tG-Loss: [11.12654, 2.9418597, 11.123598]\tD-Loss: [0.00755188 0.99972534]\n",
      "29 time: 1:44:40.628834\n",
      "\tG-Loss: [11.266906, 2.8946, 11.264011]\tD-Loss: [0.00769703 0.9996948 ]\n",
      "30 time: 1:48:50.863640\n",
      "\tG-Loss: [11.402759, 2.960262, 11.399798]\tD-Loss: [0.00814709 0.9996643 ]\n",
      "31 time: 1:53:47.474857\n",
      "\tG-Loss: [11.631714, 2.911562, 11.628802]\tD-Loss: [0.0083584 0.9998169]\n",
      "32 time: 1:57:59.821159\n",
      "\tG-Loss: [11.216337, 3.0380268, 11.213299]\tD-Loss: [0.0066496 0.9998779]\n",
      "33 time: 2:02:18.081263\n",
      "\tG-Loss: [11.264356, 3.0220742, 11.261333]\tD-Loss: [0.0064077 0.9998779]\n",
      "34 time: 2:06:43.991457\n",
      "\tG-Loss: [11.429505, 3.0516164, 11.426454]\tD-Loss: [0.00606557 0.99993896]\n",
      "35 time: 2:11:19.280818\n",
      "\tG-Loss: [11.914743, 3.0282657, 11.9117155]\tD-Loss: [0.00688093 0.99975586]\n",
      "36 time: 2:16:39.666539\n",
      "\tG-Loss: [12.441203, 2.98005, 12.438223]\tD-Loss: [0.00722453 0.99972534]\n",
      "37 time: 2:21:09.090791\n",
      "\tG-Loss: [11.752438, 3.0307894, 11.749407]\tD-Loss: [0.00762783 0.9998474 ]\n",
      "38 time: 2:25:48.008277\n",
      "\tG-Loss: [11.19454, 3.1276443, 11.191412]\tD-Loss: [0.00566829 0.99993896]\n",
      "39 time: 2:30:31.636850\n",
      "\tG-Loss: [10.91272, 3.1913898, 10.909529]\tD-Loss: [0.00447521 1.        ]\n",
      "40 time: 2:35:22.391968\n",
      "\tG-Loss: [10.732702, 3.2281947, 10.729474]\tD-Loss: [0.00447296 0.99993896]\n",
      "41 time: 2:41:08.043666\n",
      "\tG-Loss: [10.6118965, 3.2162838, 10.60868]\tD-Loss: [0.00400335 0.9999695 ]\n",
      "42 time: 2:46:03.704358\n",
      "\tG-Loss: [10.508872, 3.230095, 10.505642]\tD-Loss: [0.00413902 1.        ]\n",
      "43 time: 2:51:01.204148\n",
      "\tG-Loss: [10.432947, 3.243324, 10.429704]\tD-Loss: [0.00405057 1.        ]\n",
      "44 time: 2:56:07.604932\n",
      "\tG-Loss: [10.373684, 3.2327695, 10.370451]\tD-Loss: [0.00397419 1.        ]\n",
      "45 time: 3:01:20.216579\n",
      "\tG-Loss: [10.332813, 3.241272, 10.329572]\tD-Loss: [0.00398233 1.        ]\n",
      "46 time: 3:07:24.202482\n",
      "\tG-Loss: [10.353313, 3.2371483, 10.350077]\tD-Loss: [0.00400739 0.9999695 ]\n",
      "47 time: 3:12:33.790456\n",
      "\tG-Loss: [10.335111, 3.2617328, 10.331849]\tD-Loss: [0.00399935 1.        ]\n",
      "48 time: 3:17:48.434860\n",
      "\tG-Loss: [10.43805, 3.2317786, 10.434818]\tD-Loss: [0.00394905 1.        ]\n",
      "49 time: 3:23:11.855965\n",
      "\tG-Loss: [10.430433, 3.2908196, 10.427142]\tD-Loss: [0.00385929 1.        ]\n",
      "50 time: 3:28:39.706556\n",
      "\tG-Loss: [10.62929, 3.2317035, 10.626058]\tD-Loss: [0.00405452 0.99990845]\n",
      "51 time: 3:35:06.484903\n",
      "\tG-Loss: [11.033572, 3.2780178, 11.030294]\tD-Loss: [0.00419311 1.        ]\n",
      "52 time: 3:40:37.090652\n",
      "\tG-Loss: [10.543851, 3.2387054, 10.540612]\tD-Loss: [0.00373515 1.        ]\n",
      "53 time: 3:46:12.739054\n",
      "\tG-Loss: [10.446206, 3.294275, 10.442912]\tD-Loss: [0.00366278 0.99993896]\n",
      "54 time: 3:51:53.144164\n",
      "\tG-Loss: [10.300188, 3.2948458, 10.296893]\tD-Loss: [0.00339817 1.        ]\n",
      "55 time: 3:57:44.593368\n",
      "\tG-Loss: [10.173686, 3.3237464, 10.170362]\tD-Loss: [0.00321019 1.        ]\n",
      "56 time: 4:04:31.468114\n",
      "\tG-Loss: [10.042386, 3.3277469, 10.039059]\tD-Loss: [0.00304064 0.9999695 ]\n",
      "57 time: 4:10:17.294736\n",
      "\tG-Loss: [9.92216, 3.3392348, 9.918821]\tD-Loss: [0.00284841 1.        ]\n",
      "58 time: 4:16:07.737866\n",
      "\tG-Loss: [9.848081, 3.361444, 9.844719]\tD-Loss: [0.00300361 1.        ]\n",
      "59 time: 4:22:01.238867\n",
      "\tG-Loss: [9.801234, 3.368446, 9.797866]\tD-Loss: [0.00284745 1.        ]\n",
      "60 time: 4:28:06.002575\n",
      "\tG-Loss: [9.771959, 3.3860974, 9.768573]\tD-Loss: [0.00287483 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Init model...\")\n",
    "gan = SRGAN(num_gpus=8, weights_path='./')\n",
    "print(\"Starting training...\")\n",
    "gan.train(epochs=1000, batch_size=4, sample_interval=5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Keras OvrSR_V2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
