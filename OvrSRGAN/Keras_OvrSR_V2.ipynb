{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mihirsathe/ECE285FA18_BestGroup/blob/master/Keras_OvrSR_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "id": "etiSuCAVNfhL",
    "outputId": "8fb95154-1820-4416-8892-5d1cf8ff4113"
   },
   "outputs": [],
   "source": [
    "#!pip3 install -q git+https://www.github.com/keras-team/keras-contrib.git\n",
    "#!pip3 install -q DataLoader\n",
    "#!pip3 install -q imageio\n",
    "#!pip3 install -q keras\n",
    "\n",
    "#Mount google drive with dataset\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "dir_pfx = './'\n",
    "data_dir = '../../data/Vehicules1024/'\n",
    "# keras.backend.set_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xaMsQct9Lxem",
    "outputId": "2857e543-c53c-4e80-858f-73e41622191e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Super-resolution of CelebA using Generative Adversarial Networks.\n",
    "# The dataset can be downloaded from: https://www.dropbox.com/sh/8oqt9vytwxb3s4r/AADIKlz8PR9zr6Y20qbkunrba/Img/img_align_celeba.zip?dl=0\n",
    "# Instrustion on running the script:\n",
    "# 1. Download the dataset from the provided link\n",
    "# 2. Save the folder 'img_align_celeba' to 'datasets/'\n",
    "# 4. Run the sript using command 'python srgan.py'\n",
    "\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import scipy\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Add\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.applications import VGG19\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# from google.col\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import keras.backend as K\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M6s0jXbDLqgV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "from keras.utils import Sequence\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from skimage.transform import downscale_local_mean\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "  return image/255.0    \n",
    "\n",
    "def read_VEDAI(subset, PATH_TO_VEHICLES_FOLDER):\n",
    "    # Takes in the full path to the unzipped \"VEHICULES\" folder\n",
    "    # Returns mapping dict, RGB and Infrared images in \n",
    "    # (images, x, y, channels) format,\n",
    "    # saves a txt file with mapping of rgb/infra idx to filename\n",
    "    # NUM_FILES = 2536\n",
    "    # MAX_INDEX = 1272\n",
    "    X_PIXELS = 1024\n",
    "    Y_PIXELS = 1024\n",
    "    PATH_TO_VEHICLES_FOLDER = PATH_TO_VEHICLES_FOLDER#[0]\n",
    "\n",
    "    onlyfiles = [f for f in listdir(PATH_TO_VEHICLES_FOLDER) if isfile(\n",
    "        join(PATH_TO_VEHICLES_FOLDER, f)) and \"png\" in f]\n",
    "    # assert len(onlyfiles) == NUM_FILES, \"Not the full VEDAI 1024 Dataset\"\n",
    "    rgb = np.zeros((len(subset), X_PIXELS, Y_PIXELS, 3))\n",
    "    infra = np.zeros((len(subset), X_PIXELS, Y_PIXELS, 1))\n",
    "    indices = subset\n",
    "\n",
    "    print(indices)\n",
    "    print(rgb.shape)\n",
    "    index_filename_map = {}\n",
    "    im_cnt = 0\n",
    "\n",
    "    for index in indices:\n",
    "        pair = [file for file in onlyfiles if str(index) in file]\n",
    "        if pair:\n",
    "            for file in pair:\n",
    "                index_filename_map[im_cnt] = file.split('_')[0]\n",
    "                im = imageio.imread(PATH_TO_VEHICLES_FOLDER + '/' + file)\n",
    "                if \"co\" in file:\n",
    "                    # print('Inserting RGB @ '+ str(im_cnt))\n",
    "                    rgb[im_cnt, :, :, :] = np.reshape(\n",
    "                        im, (tuple([1]) + im.shape))\n",
    "                elif \"ir\" in file:\n",
    "                    # print('Inserting Infra @ '+ str(im_cnt))\n",
    "                    infra[im_cnt, :, :, :] = np.reshape(\n",
    "                        im, (tuple([1]) + im.shape + tuple([1])))\n",
    "            im_cnt = im_cnt + 1\n",
    "        else:\n",
    "            print(\"The following image is missing!: \" + index)\n",
    "    # print(index_filename_map)\n",
    "    f = open(PATH_TO_VEHICLES_FOLDER + \"_mapping.txt\", \"w\")\n",
    "    f.write(str(index_filename_map))\n",
    "    f.close()\n",
    "    return rgb, infra\n",
    "\n",
    "\n",
    "def scan_dataset(PATH_TO_VEHICLES_FOLDER, number_of_imgs):\n",
    "    # Takes in the full path to the unzipped \"VEHICULES\" folder\n",
    "    # Returns a list of all the files\n",
    "    # and saves a dataset summary text file with list of all file names\n",
    "    MAX_INDEX = number_of_imgs\n",
    "    PATH_TO_VEHICLES_FOLDER = PATH_TO_VEHICLES_FOLDER#[0]\n",
    "    indices = [format(n, '08') for n in range(MAX_INDEX)]\n",
    "    export_files = []\n",
    "\n",
    "    onlyfiles = [f for f in listdir(PATH_TO_VEHICLES_FOLDER) if isfile(\n",
    "        join(PATH_TO_VEHICLES_FOLDER, f)) and \"png\" in f]\n",
    "\n",
    "    for index in indices:\n",
    "        pair = [file for file in onlyfiles if str(index) in file]\n",
    "        if pair:\n",
    "            for file in pair:\n",
    "                if \"co\" in file:\n",
    "                    export_files.append(file.split('_')[0])\n",
    "        else:\n",
    "            print(\"The following image is missing!: \" + index)\n",
    "    np.savetxt(PATH_TO_VEHICLES_FOLDER + '_summary.txt',\n",
    "               export_files, delimiter=\" \", fmt=\"%s\")\n",
    "    return export_files\n",
    "\n",
    "\n",
    "def create_subsets(imgs, output_path, use_validation=True,\n",
    "                   training_percent=0.7, testing_percent=0.3, SEED=1):\n",
    "    # Takes a list of image file names and shuffles\n",
    "    # them before splitting them into required subsets\n",
    "    # Saves txt files containing the names of the files\n",
    "    # used in each subset, no return value\n",
    "\n",
    "    assert training_percent + \\\n",
    "        testing_percent == 1, \"Training + testing percents must equal 1.\"\n",
    "    random.seed(SEED)\n",
    "    random.shuffle(imgs)\n",
    "    print('Using ' + str(len(imgs)) + ' images.')\n",
    "    print('Saving files to ' + output_path)\n",
    "    if not use_validation:\n",
    "        training_imgs = imgs[:int(len(imgs) * training_percent)]\n",
    "        testing_imgs = imgs[int(len(imgs) * training_percent):]\n",
    "        np.savetxt(output_path + 'training.txt',\n",
    "                   training_imgs, delimiter=\" \", fmt=\"%s\")\n",
    "        np.savetxt(output_path + 'testing.txt',\n",
    "                   testing_imgs, delimiter=\" \", fmt=\"%s\")\n",
    "        return training_imgs, testing_imgs\n",
    "    else:\n",
    "        validation_split = 0.3  # use 30% of training dataset for validation\n",
    "        training_imgs = imgs[int(len(imgs) * validation_split *\n",
    "                                 training_percent):int(len(imgs) *\n",
    "                                                       training_percent)]\n",
    "        validation_imgs = imgs[:int(\n",
    "            len(imgs) * validation_split * training_percent)]\n",
    "        testing_imgs = imgs[int(len(imgs) * training_percent):]\n",
    "        np.savetxt(output_path + 'validation.txt',\n",
    "                   validation_imgs, delimiter=\" \", fmt=\"%s\")\n",
    "        np.savetxt(output_path + 'training.txt',\n",
    "                   testing_imgs, delimiter=\" \", fmt=\"%s\")\n",
    "        np.savetxt(output_path + 'testing.txt',\n",
    "                   testing_imgs, delimiter=\" \", fmt=\"%s\")\n",
    "        return training_imgs, validation_imgs, testing_imgs\n",
    "\n",
    "\n",
    "def save_VEDAI(rgb, infra):\n",
    "    # Takes in arrays of rgb and infrared images\n",
    "    # Saves them to disk, no return value\n",
    "    np.save(\"vedai_rgb_all.npy\", rgb)\n",
    "    np.save(\"vedai_infra_all.npy\", infra)\n",
    "\n",
    "\n",
    "def load_VEDAI():\n",
    "    # No parameters, expected to run in directory with VEDAI.npy files\n",
    "    # Returns two arrays with rgb and infrared images respectively\n",
    "    rgb = np.load(\"vedai_rgb_all.npy\")\n",
    "    infra = np.load(\"vedai_infra_all.npy\")\n",
    "    return rgb, infra\n",
    "\n",
    "\n",
    "def data_explore(data):\n",
    "    print(\"Shape of the data is\" + str(data.shape))\n",
    "    print(\"Dtype of the data is\" + str(data.dtype))\n",
    "\n",
    "\n",
    "def combine_rgb_infra(rgb, infra):\n",
    "    # Concatenates the two modalities along the channels axis\n",
    "    four_channel = np.concatenate((rgb, infra), axis=-1)\n",
    "    return four_channel\n",
    "\n",
    "  \n",
    "def overlapping_patches(images, patch_size=(64, 64), padding=\"VALID\"):\n",
    "    sess = tf.Session()\n",
    "\n",
    "    num_images, size_x, size_y, channels = images.shape\n",
    "    ims = tf.convert_to_tensor(images)\n",
    "    patch_x, patch_y = patch_size\n",
    "    patches = tf.extract_image_patches(ims, [1, patch_x, patch_y, 1], [\n",
    "        1, patch_x, patch_y, 1], [1, 1, 1, 1], padding=padding)\n",
    "    patches_shape = tf.shape(patches)\n",
    "    with sess.as_default():\n",
    "        np = tf.reshape(patches, [tf.reduce_prod(patches_shape[0:3]),\n",
    "                                  patch_x, patch_y, channels]).eval()\n",
    "        return np  \n",
    "\n",
    "\n",
    "def non_overlapping_patches(image, patch_size=(64, 64)):\n",
    "    size_x, size_y, channels = image.shape\n",
    "    patch_x, patch_y = patch_size\n",
    "    im_pad = np.pad(image, ((0, patch_x - size_x % patch_x),\n",
    "                            (0, patch_y - size_y % patch_y), (0, 0)),\n",
    "                    mode=\"constant\")\n",
    "    if size_x % patch_x == 0 and size_y % patch_y == 0:\n",
    "        im_pad = image\n",
    "    pad_x, pad_y, channels = im_pad.shape\n",
    "    print(im_pad.shape)\n",
    "    num_patches = (pad_x // patch_x) * (pad_y // patch_y)\n",
    "    patches = np.zeros((num_patches, patch_x, patch_y, channels))\n",
    "    counter = 0\n",
    "    for i in range((pad_x // patch_x)):\n",
    "        for j in range((pad_y // patch_y)):\n",
    "            x_s = i * patch_x\n",
    "            y_s = j * patch_y\n",
    "            patches[counter, :, :, :] = im_pad[x_s:x_s + patch_x,\n",
    "                                               y_s:y_s + patch_y, :]\n",
    "            counter += 1\n",
    "    return patches\n",
    "\n",
    "\n",
    "def downsample_image(image, block=(4, 4, 1)):\n",
    "    # Downsamples numpy array image by factor\n",
    "    # Returns  the downsampled copy\n",
    "    if image.ndim == 4:\n",
    "      block=(1, 4, 4, 1)\n",
    "    return downscale_local_mean(image, block)\n",
    "\n",
    "\n",
    "def reconstruct_patches(patches, image_size):\n",
    "    # TODO: Create a function which reconstructs an image\n",
    "    # when given patches created by non_overlapping_patches\n",
    "    # Discards predictions for zero border\n",
    "    pass\n",
    "\n",
    "def get_images_to_four_chan(img_name, DATASET_PATH, ch_num=4):\n",
    "  #co = imageio.imread(DATASET_PATH + 'VEDAI/' + img_name + '_co.png')\n",
    "  co = imageio.imread(DATASET_PATH + img_name + '_co.png')\n",
    "  if ch_num == 4:\n",
    "    #ir = imageio.imread(DATASET_PATH + 'VEDAI/' + img_name + '_ir.png')\n",
    "    ir = imageio.imread(DATASET_PATH + img_name + '_ir.png')\n",
    "    rgb = np.reshape(co, (tuple([1]) + co.shape))\n",
    "    infra = np.reshape(ir, (tuple([1]) + ir.shape + tuple([1])))  \n",
    "    return combine_rgb_infra(rgb, infra)\n",
    "  elif ch_num == 3:\n",
    "    return np.reshape(co, (tuple([1]) + co.shape))\n",
    "\n",
    "def load_data(file_idx, txt_file, DATASET_PATH, batch_size=1):\n",
    "  # read in batch of file names from txt file with randomized filenames\n",
    "  # return the lr and hr patches\n",
    "  \n",
    "  #read x lines from txt file\n",
    "  text_file = open(DATASET_PATH +\"/training.txt\", \"r\")\n",
    "  img_files = text_file.read().split('\\n')\n",
    "  text_file.close()\n",
    "\n",
    "# TODO: preallocate arrays for speed  \n",
    "  # Number of patches * ims_per_batch\n",
    "  batchsz = 256 * batch_size\n",
    "  # RGB\n",
    "  channels = 3\n",
    "  # Default patch size\n",
    "  patch_x, patch_y = 64, 64\n",
    "\n",
    "  # Preallocate arrays of the correct size\n",
    "  imgs_hr = np.zeros((batchsz, patch_x, patch_y, channels))\n",
    "  \n",
    "  # Batch number * ims_per_batch\n",
    "  start = file_idx\n",
    "  end = file_idx + batch_size\n",
    "      \n",
    "#   print(start, end)    \n",
    "  \n",
    "  im_num = 0\n",
    "  for i in range(start, end):\n",
    "      st, stp = im_num * 256, (im_num + 1) * 256\n",
    "      im_num += 1 \n",
    "      patch = overlapping_patches(\n",
    "          normalize(get_images_to_four_chan(img_files[i], DATASET_PATH, channels)))\n",
    "      imgs_hr[st:stp, :, :, :] = patch\n",
    "      \n",
    "  imgs_lr = np.asarray([downsample_image(patch) for patch in imgs_hr])\n",
    "      \n",
    "  file_idx = file_idx + batch_size # update current file_idx\n",
    "  return imgs_hr, imgs_lr, file_idx\n",
    "\n",
    "\n",
    "class VEDAISequence(Sequence):\n",
    "\n",
    "    def __init__(self, rgb, infra, ims_per_batch):\n",
    "        self.r, self.i = rgb, infra\n",
    "        self.ims_per_batch = ims_per_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns number of batches given training set and ims_per_batch\n",
    "        return int(np.ceil(len(self.r) / float(self.ims_per_batch)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Number of patches * ims_per_batch\n",
    "        batchsz = 256 * self.ims_per_batch\n",
    "        # RGB and infra\n",
    "        channels = 4\n",
    "        # Default patch size\n",
    "        patch_x, patch_y = 64, 64\n",
    "\n",
    "        # Batch number * ims_per_batch\n",
    "        start = idx * self.ims_per_batch\n",
    "        # Batch number * ims_per_batch  + 1\n",
    "        end = (idx + 1) * self.ims_per_batch\n",
    "\n",
    "        # Preallocate arrays of the correct size\n",
    "        high_res = np.zeros((batchsz, patch_x, patch_y, channels))\n",
    "        im_num = 0\n",
    "        for ind in range(start, end):\n",
    "            st, stp = im_num * 256, (im_num + 1) * 256\n",
    "            im_num += 1\n",
    "            rgb = imageio.imread(self.r[ind])\n",
    "            infra = imageio.imread(self.i[ind])\n",
    "            high_res[st:stp, :, :, :] = non_overlapping_patches(\n",
    "                combine_rgb_infra(rgb, infra))\n",
    "        low_res = np.asarray([downsample_image(patch) for patch in high_res])\n",
    "        return low_res, high_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "uwrcfc4buWYf",
    "outputId": "ae18736f-0dee-428b-b5bf-2b6933141954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following image is missing!: 00000005\n",
      "The following image is missing!: 00000023\n",
      "Using 48 images.\n",
      "Saving files to ../../data/Vehicules1024/\n"
     ]
    }
   ],
   "source": [
    "#path = glob.glob('./drive/My Drive/ECE285_Proj/datasets/VEDAI/*')\n",
    "#files = scan_dataset(path, 25)\n",
    "files = scan_dataset(data_dir, 50) #TODO: 50\n",
    "#training_set, testing_set = create_subsets(files,'./drive/My Drive/ECE285_Proj/datasets/VEDAI/', use_validation = False)\n",
    "training_set, testing_set = create_subsets(files,data_dir, use_validation = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "colab_type": "code",
    "id": "zD75vf6rLotT",
    "outputId": "a46355fa-5a7b-4fcd-a6a1-2db55d84080b"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SRGAN():\n",
    "    def __init__(self,num_gpus=None, weights_path=None):\n",
    "        # Input shape\n",
    "        self.channels = 3                  # RGB & Infra\n",
    "        self.lr_height = 16                 # Low resolution height\n",
    "        self.lr_width = 16                 # Low resolution width\n",
    "        self.lr_shape = (self.lr_height, self.lr_width, self.channels)\n",
    "        self.hr_height = self.lr_height*4   # High resolution height\n",
    "        self.hr_width = self.lr_width*4     # High resolution width\n",
    "        self.hr_shape = (self.hr_height, self.hr_width, self.channels)\n",
    "        self.num_gpus = num_gpus\n",
    "        self.weights_path = weights_path\n",
    "\n",
    "\n",
    "        # Number of residual blocks in the generator\n",
    "        self.n_residual_blocks = 16\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # We use a pre-trained VGG19 model to extract image features from the high resolution\n",
    "        # and the generated high resolution images and minimize the mse between them\n",
    "        self.vgg = self.build_vgg()\n",
    "        self.vgg.trainable = False\n",
    "        self.vgg.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.hr_height / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 16\n",
    "        self.df = 16\n",
    "\n",
    "        # Build and compile the discriminator (Keep off GPU)\n",
    "        self.disc_model = self.build_discriminator()\n",
    "        \n",
    "        if self.num_gpus is not None:\n",
    "            self.discriminator = multi_gpu_model(self.disc_model, gpus=self.num_gpus)\n",
    "        \n",
    "        self.discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        \n",
    "        #Configure data loader name, is this still needed?\n",
    "        self.dataset_name = 'VEDAI'\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # High res. and low res. images\n",
    "        img_hr = Input(shape=self.hr_shape)\n",
    "        img_lr = Input(shape=self.lr_shape)\n",
    "\n",
    "        # Generate high res. version from low res.\n",
    "        fake_hr = self.generator(img_lr)\n",
    "\n",
    "        # Extract image features of the generated img\n",
    "        fake_features = self.vgg(fake_hr)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Discriminator determines validity of generated high res. images\n",
    "        validity = self.discriminator(fake_hr)\n",
    "\n",
    "        self.combined = Model([img_lr, img_hr], [validity, fake_features])\n",
    "        \n",
    "        if self.num_gpus is not None:\n",
    "            self.combined = multi_gpu_model(self.combined, gpus=self.num_gpus)       \n",
    "        \n",
    "        self.combined.compile(loss=['binary_crossentropy', 'mse'],\n",
    "                              loss_weights=[1e-3, 1],\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "    def build_vgg(self):\n",
    "        \"\"\"\n",
    "        Builds a pre-trained VGG19 model that outputs image features extracted at the\n",
    "        third block of the model\n",
    "        \"\"\"\n",
    "        vgg = VGG19(weights='imagenet')\n",
    "        # Set outputs to outputs of last conv. layer in block 3\n",
    "        # See architecture at: https://github.com/keras-team/keras/blob/master/keras/applications/vgg19.py\n",
    "        vgg.outputs = [vgg.layers[9].output]\n",
    "\n",
    "        img = Input(shape=self.hr_shape)\n",
    "       \n",
    "        # Extract image features\n",
    "        img_features = vgg(img)\n",
    "\n",
    "        return Model(img, img_features)\n",
    "\n",
    "    def build_generator(self):\n",
    "    \n",
    "        def residual_block(layer_input, filters):\n",
    "            \"\"\"Residual block described in paper\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=3, strides=1, padding='same')(layer_input)\n",
    "            d = Activation('relu')(d)\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Conv2D(filters, kernel_size=3, strides=1, padding='same')(d)\n",
    "            d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Add()([d, layer_input])\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(64, kernel_size=3, strides=1, padding='same')(u)\n",
    "            u = Activation('relu')(u)\n",
    "            return u\n",
    "\n",
    "        # Low resolution image input\n",
    "        img_lr = Input(shape=self.lr_shape)\n",
    "\n",
    "        # Pre-residual block\n",
    "        c1 = Conv2D(16, kernel_size=9, strides=1, padding='same')(img_lr)\n",
    "        c1 = Activation('relu')(c1)\n",
    "\n",
    "        # Propogate through residual blocks\n",
    "        r = residual_block(c1, self.gf)\n",
    "        for _ in range(self.n_residual_blocks - 1):\n",
    "            r = residual_block(r, self.gf)\n",
    "\n",
    "        # Post-residual block\n",
    "        c2 = Conv2D(16, kernel_size=3, strides=1, padding='same')(r)\n",
    "        c2 = BatchNormalization(momentum=0.8)(c2)\n",
    "        c2 = Add()([c2, c1])\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(c2)\n",
    "        u2 = deconv2d(u1)\n",
    "\n",
    "        # Generate high resolution output\n",
    "        gen_hr = Conv2D(self.channels, kernel_size=9, strides=1, padding='same', activation='tanh')(u2)\n",
    "        \n",
    "        generator = Model(img_lr, gen_hr)\n",
    "        \n",
    "        if self.weights_path is not None:\n",
    "            print('Loading generator weights...')\n",
    "            generator.load_weights(self.weights_path + 'OvrSR_generator_weights.h5')\n",
    "            print('Loaded...')\n",
    "            \n",
    "        return Model(img_lr, gen_hr)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_block(layer_input, filters, strides=1, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=3, strides=strides, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        # Input img\n",
    "        d0 = Input(shape=self.hr_shape)\n",
    "\n",
    "        d1 = d_block(d0, self.df, bn=False)\n",
    "        d2 = d_block(d1, self.df, strides=2)\n",
    "        d3 = d_block(d2, self.df*2)\n",
    "        d4 = d_block(d3, self.df*2, strides=2)\n",
    "        d5 = d_block(d4, self.df*4)\n",
    "        d6 = d_block(d5, self.df*4, strides=2)\n",
    "        d7 = d_block(d6, self.df*8)\n",
    "        d8 = d_block(d7, self.df*8, strides=2)\n",
    "\n",
    "        d9 = Dense(self.df*4)(d8)\n",
    "        d10 = LeakyReLU(alpha=0.2)(d9)\n",
    "        validity = Dense(1, activation='sigmoid')(d10)\n",
    "\n",
    "        return Model(d0, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ----------------------\n",
    "            #  Train Discriminator\n",
    "            # ----------------------\n",
    "\n",
    "            # Sample images and their conditioning counterparts\n",
    "            batch_idx = 0\n",
    "            imgs_hr, imgs_lr, batch_idx = load_data(batch_idx, training_set, data_dir, batch_size)\n",
    "\n",
    "            # From low res. image generate high res. version\n",
    "            fake_hr = self.generator.predict(imgs_lr)\n",
    "\n",
    "            valid = np.ones((imgs_hr.shape[0],) + self.disc_patch)\n",
    "            fake = np.zeros((imgs_hr.shape[0],) + self.disc_patch)\n",
    "\n",
    "            # Train the discriminators (original images = real / generated = Fake)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs_hr, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(fake_hr, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generator\n",
    "            # ------------------\n",
    "\n",
    "            # Sample images and their conditioning counterparts\n",
    "            imgs_hr, imgs_lr, batch_idx = load_data(batch_idx, training_set,\n",
    "                      data_dir,\n",
    "                      batch_size)\n",
    "    \n",
    "            # The generators want the discriminators to label the generated images as real\n",
    "            valid = np.ones((imgs_hr.shape[0],) + self.disc_patch)\n",
    "\n",
    "            # Extract ground truth image features using pre-trained VGG19 model\n",
    "            image_features = self.vgg.predict(imgs_hr)\n",
    "\n",
    "            # Train the generators\n",
    "            g_loss = self.combined.train_on_batch([imgs_lr, imgs_hr], [valid, image_features])\n",
    "\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "            # Plot the progress\n",
    "            print (\"%d time: %s\" % (epoch, elapsed_time))\n",
    "            print(\"\\tG-Loss: \" + str(g_loss) + \"\\tD-Loss: \" + str(d_loss))\n",
    "            \n",
    "            # Append current losses and save\n",
    "            \n",
    "            losses.append([epoch] + list(g_loss) + list(d_loss))\n",
    "            \n",
    "            np.save(dir_pfx + 'loss_logging/loss_log.npy', arr=np.array(losses))            \n",
    "            \n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "                self.generator.save(self.weights_path + 'OvrSR_generator_weights.h5')\n",
    "                self.disc_model.save(self.weights_path + 'OvrSR_discriminator_weights.h5')\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        os.makedirs(dir_pfx + 'images/{0}'.format(self.dataset_name), exist_ok=True)\n",
    "        r, c = 2, 2\n",
    "        \n",
    "        batch_idx = 0\n",
    "        #imgs_hr, imgs_lr, batch_idx = load_data(batch_idx, testing_set, './drive/My Drive/ECE285_Proj/datasets/VEDAI/')\n",
    "        imgs_hr, imgs_lr, batch_idx = load_data(batch_idx, testing_set, data_dir)\n",
    "        fake_hr = self.generator.predict(imgs_lr)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        imgs_lr = 0.5 * imgs_lr + 0.5\n",
    "        fake_hr = 0.5 * fake_hr + 0.5\n",
    "        imgs_hr = 0.5 * imgs_hr + 0.5\n",
    "\n",
    "        # Save generated images and the high resolution originals\n",
    "        titles = ['Generated', 'Original']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        fig.suptitle('Epoch %d' % (epoch), fontsize=16)\n",
    "        for row in range(r):\n",
    "            for col, image in enumerate([fake_hr, imgs_hr]):\n",
    "                axs[row, col].imshow(image[row])\n",
    "                axs[row, col].set_title(titles[col])\n",
    "                axs[row, col].axis('off')\n",
    "            cnt += 1\n",
    "        #fig.savefig(\"./drive/My Drive/ECE285_Proj/images/%s%d.png\" % (self.dataset_name, epoch))\n",
    "        fig.savefig(dir_pfx + 'images/{0}{1}.png'.format(self.dataset_name, epoch))\n",
    "        plt.close()\n",
    "\n",
    "        # Save low resolution images for comparison\n",
    "        for i in range(r):\n",
    "            fig = plt.figure()\n",
    "            plt.imshow(imgs_lr[i])\n",
    "            #fig.savefig('./drive/My Drive/ECE285_Proj/images/%s/%d_lowres%d.png' % (self.dataset_name, epoch, i))\n",
    "            fig.savefig(dir_pfx + 'images/{0}/{1}_lowres{2}.png'.format(self.dataset_name, epoch, i))\n",
    "            plt.close()\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    print(\"Init model...\")\n",
    "#    gan = SRGAN(num_gpus=8, weights_path='./')\n",
    "#    print(\"Starting training...\")\n",
    "#    gan.train(epochs=1000, batch_size=4, sample_interval=5)\n",
    "#     gan.train(epochs=300, batch_size=1, sample_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init model...\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[25088,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node fc1_1/random_uniform/RandomUniform (defined at /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4139)  = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=6918704, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](fc1_1/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'fc1_1/random_uniform/RandomUniform', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1424, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-954617087a9b>\", line 2, in <module>\n    gan = SRGAN(num_gpus=3, weights_path='./weights/')\n  File \"<ipython-input-8-d91eba24f5c9>\", line 23, in __init__\n    self.vgg = self.build_vgg()\n  File \"<ipython-input-8-d91eba24f5c9>\", line 84, in build_vgg\n    vgg = VGG19(weights='imagenet')\n  File \"/usr/local/lib/python3.5/dist-packages/keras/applications/__init__.py\", line 28, in wrapper\n    return base_fun(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/applications/vgg19.py\", line 11, in VGG19\n    return vgg19.VGG19(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras_applications/vgg19.py\", line 190, in VGG19\n    x = layers.Dense(4096, activation='relu', name='fc1')(x)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\", line 431, in __call__\n    self.build(unpack_singleton(input_shapes))\n  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/core.py\", line 866, in build\n    constraint=self.kernel_constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\", line 249, in add_weight\n    weight = K.variable(initializer(shape),\n  File \"/usr/local/lib/python3.5/dist-packages/keras/initializers.py\", line 218, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 4139, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/random_ops.py\", line 243, in random_uniform\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_random_ops.py\", line 733, in random_uniform\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[25088,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node fc1_1/random_uniform/RandomUniform (defined at /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4139)  = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=6918704, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](fc1_1/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[25088,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node fc1_1/random_uniform/RandomUniform}} = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=6918704, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](fc1_1/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-954617087a9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Init model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSRGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./weights/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-d91eba24f5c9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_gpus, weights_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# We use a pre-trained VGG19 model to extract image features from the high resolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# and the generated high resolution images and minimize the mse between them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         self.vgg.compile(loss='mse',\n",
      "\u001b[0;32m<ipython-input-8-d91eba24f5c9>\u001b[0m in \u001b[0;36mbuild_vgg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mthird\u001b[0m \u001b[0mblock\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mvgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Set outputs to outputs of last conv. layer in block 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# See architecture at: https://github.com/keras-team/keras/blob/master/keras/applications/vgg19.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/applications/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utils'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/applications/vgg19.py\u001b[0m in \u001b[0;36mVGG19\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mkeras_modules_injection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mVGG19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvgg19\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVGG19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras_applications/vgg19.py\u001b[0m in \u001b[0;36mVGG19\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mcache_subdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 file_hash='253f8cb515780f3b799900260a226db6')\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'theano'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mkeras_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_all_kernels_in_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1166\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1056\u001b[0m                              ' elements.')\n\u001b[1;32m   1057\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2468\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2469\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2470\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[25088,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node fc1_1/random_uniform/RandomUniform (defined at /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4139)  = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=6918704, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](fc1_1/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'fc1_1/random_uniform/RandomUniform', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.5/asyncio/base_events.py\", line 1424, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.5/asyncio/events.py\", line 126, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-954617087a9b>\", line 2, in <module>\n    gan = SRGAN(num_gpus=3, weights_path='./weights/')\n  File \"<ipython-input-8-d91eba24f5c9>\", line 23, in __init__\n    self.vgg = self.build_vgg()\n  File \"<ipython-input-8-d91eba24f5c9>\", line 84, in build_vgg\n    vgg = VGG19(weights='imagenet')\n  File \"/usr/local/lib/python3.5/dist-packages/keras/applications/__init__.py\", line 28, in wrapper\n    return base_fun(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/applications/vgg19.py\", line 11, in VGG19\n    return vgg19.VGG19(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras_applications/vgg19.py\", line 190, in VGG19\n    x = layers.Dense(4096, activation='relu', name='fc1')(x)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\", line 431, in __call__\n    self.build(unpack_singleton(input_shapes))\n  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/core.py\", line 866, in build\n    constraint=self.kernel_constraint)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\", line 249, in add_weight\n    weight = K.variable(initializer(shape),\n  File \"/usr/local/lib/python3.5/dist-packages/keras/initializers.py\", line 218, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 4139, in random_uniform\n    dtype=dtype, seed=seed)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/random_ops.py\", line 243, in random_uniform\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_random_ops.py\", line 733, in random_uniform\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[25088,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node fc1_1/random_uniform/RandomUniform (defined at /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4139)  = RandomUniform[T=DT_INT32, dtype=DT_FLOAT, seed=87654321, seed2=6918704, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](fc1_1/random_uniform/shape)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "print(\"Init model...\")\n",
    "gan = SRGAN(num_gpus=3, weights_path='./weights/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "gan.train(epochs=100, batch_size=4, sample_interval=5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Keras OvrSR_V2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
